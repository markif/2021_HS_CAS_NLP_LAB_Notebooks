{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"400\" src=\"https://www.fhnw.ch/de/++theme++web16theme/assets/media/img/fachhochschule-nordwestschweiz-fhnw-logo.svg\" alt=\"FHNW Logo\">\n",
    "\n",
    "\n",
    "# Data Augmentation with Back Translation using Transformers\n",
    "\n",
    "by Fabian Märki\n",
    "\n",
    "## Summary\n",
    "The aim of this notebook is to show how Huggingface's model can be used for back translation.\n",
    "\n",
    "### Sources\n",
    "- [Text Data Augmentation with Back TranslationPermalink](https://amitness.com/back-translation/)\n",
    "- [Faster batch translation](https://github.com/huggingface/transformers/issues/9994) with code example\n",
    "\n",
    "### Libraries/Models\n",
    "- [Hugging Face](https://huggingface.co)\n",
    "- [Translation Models](https://huggingface.co/models?language=de&pipeline_tag=translation&sort=downloads&search=Helsinki-NLP) that can be used with this code\n",
    "\n",
    "This notebook contains assigments: <font color='red'>Questions are written in red.</font>\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/markif/2021_HS_CAS_NLP_LAB_Notebooks/blob/master/06_b_Augmentation_with_Back_Translation_using_Transformers.ipynb\">\n",
    "  <img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install 'fhnw-nlp-utils>=0.2.13,<0.3.0'\n",
    "\n",
    "from fhnw.nlp.utils.processing import parallelize_dataframe\n",
    "from fhnw.nlp.utils.processing import is_iterable\n",
    "from fhnw.nlp.utils.storage import download\n",
    "from fhnw.nlp.utils.storage import save_dataframe\n",
    "from fhnw.nlp.utils.storage import load_dataframe\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make sure that a GPU is available (see [here](https://www.tutorialspoint.com/google_colab/google_colab_using_free_gpu.htm))!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS name: posix\n",
      "Platform name: Linux\n",
      "Platform release: 5.11.0-40-generic\n",
      "Python version: 3.6.9\n",
      "Tensorflow version: 2.5.1\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "from fhnw.nlp.utils.system import system_info\n",
    "print(system_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.63 s, sys: 1.41 s, total: 9.04 s\n",
      "Wall time: 5.12 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(350087, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "download(\"https://drive.google.com/uc?id=19AFeVnOfX8WXU4_3rM7OFoNTWWog_sb_\", \"data/german_doctor_reviews_tokenized.parq\")\n",
    "data = load_dataframe(\"data/german_doctor_reviews_tokenized.parq\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_original</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>token_clean</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>token_stem</th>\n",
       "      <th>token_clean_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ich bin franzose und bin seit ein paar Wochen ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Ich bin franzose und bin seit ein paar Wochen ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>[ich, bin, franzose, und, bin, seit, ein, paar...</td>\n",
       "      <td>ich bin franzose und bin seit ein paar wochen ...</td>\n",
       "      <td>[franzose, seit, paar, wochen, muenchen, zahn,...</td>\n",
       "      <td>[franzos, seit, paar, woch, muench, ., zahn, s...</td>\n",
       "      <td>[franzose, seit, paar, wochen, muenchen, ., za...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dieser Arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Dieser Arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "      <td>[dieser, arzt, ist, das, unmöglichste, was, mi...</td>\n",
       "      <td>dieser arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>[arzt, unmöglichste, leben, je, begegnen, unfr...</td>\n",
       "      <td>[arzt, unmog, leb, je, begegnet, unfreund, ,, ...</td>\n",
       "      <td>[arzt, unmöglichste, leben, je, begegnet, unfr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hatte akute Beschwerden am Rücken. Herr Magura...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Hatte akute Beschwerden am Rücken. Herr Magura...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>[hatte, akute, beschwerden, am, rücken, ., her...</td>\n",
       "      <td>hatte akute beschwerden am rücken . herr magur...</td>\n",
       "      <td>[akut, beschwerden, rücken, magura, erste, arz...</td>\n",
       "      <td>[akut, beschwerd, ruck, ., magura, erst, arzt,...</td>\n",
       "      <td>[akute, beschwerden, rücken, ., magura, erste,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       text_original  rating  \\\n",
       "0  Ich bin franzose und bin seit ein paar Wochen ...     2.0   \n",
       "1  Dieser Arzt ist das unmöglichste was mir in me...     6.0   \n",
       "2  Hatte akute Beschwerden am Rücken. Herr Magura...     1.0   \n",
       "\n",
       "                                                text     label  sentiment  \\\n",
       "0  Ich bin franzose und bin seit ein paar Wochen ...  positive          1   \n",
       "1  Dieser Arzt ist das unmöglichste was mir in me...  negative         -1   \n",
       "2  Hatte akute Beschwerden am Rücken. Herr Magura...  positive          1   \n",
       "\n",
       "                                         token_clean  \\\n",
       "0  [ich, bin, franzose, und, bin, seit, ein, paar...   \n",
       "1  [dieser, arzt, ist, das, unmöglichste, was, mi...   \n",
       "2  [hatte, akute, beschwerden, am, rücken, ., her...   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  ich bin franzose und bin seit ein paar wochen ...   \n",
       "1  dieser arzt ist das unmöglichste was mir in me...   \n",
       "2  hatte akute beschwerden am rücken . herr magur...   \n",
       "\n",
       "                                         token_lemma  \\\n",
       "0  [franzose, seit, paar, wochen, muenchen, zahn,...   \n",
       "1  [arzt, unmöglichste, leben, je, begegnen, unfr...   \n",
       "2  [akut, beschwerden, rücken, magura, erste, arz...   \n",
       "\n",
       "                                          token_stem  \\\n",
       "0  [franzos, seit, paar, woch, muench, ., zahn, s...   \n",
       "1  [arzt, unmog, leb, je, begegnet, unfreund, ,, ...   \n",
       "2  [akut, beschwerd, ruck, ., magura, erst, arzt,...   \n",
       "\n",
       "                               token_clean_stopwords  \n",
       "0  [franzose, seit, paar, wochen, muenchen, ., za...  \n",
       "1  [arzt, unmöglichste, leben, je, begegnet, unfr...  \n",
       "2  [akute, beschwerden, rücken, ., magura, erste,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the computed columns (will need to be re-computed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop([\"token_clean\", \"token_lemma\", \"token_stem\", \"token_clean_stopwords\", \"text_clean\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_original</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ich bin franzose und bin seit ein paar Wochen ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Ich bin franzose und bin seit ein paar Wochen ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dieser Arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Dieser Arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hatte akute Beschwerden am Rücken. Herr Magura...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Hatte akute Beschwerden am Rücken. Herr Magura...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       text_original  rating  \\\n",
       "0  Ich bin franzose und bin seit ein paar Wochen ...     2.0   \n",
       "1  Dieser Arzt ist das unmöglichste was mir in me...     6.0   \n",
       "2  Hatte akute Beschwerden am Rücken. Herr Magura...     1.0   \n",
       "\n",
       "                                                text     label  sentiment  \n",
       "0  Ich bin franzose und bin seit ein paar Wochen ...  positive          1  \n",
       "1  Dieser Arzt ist das unmöglichste was mir in me...  negative         -1  \n",
       "2  Hatte akute Beschwerden am Rücken. Herr Magura...  positive          1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep negative text (the class with fewer samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33022, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_augm = data[data[\"label\"] == \"negative\"]\n",
    "data_augm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_original</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dieser Arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Dieser Arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1. Termin:&lt;br /&gt;\\n1 Stunde Wartezimmer + 2 min...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>. Termin Stunde Wartezimmer minütige Behandlu...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Eine sehr unfreundliche Ärztin, so etwas habe ...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Eine sehr unfreundliche Ärztin, so etwas habe ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text_original  rating  \\\n",
       "1   Dieser Arzt ist das unmöglichste was mir in me...     6.0   \n",
       "13  1. Termin:<br />\\n1 Stunde Wartezimmer + 2 min...     6.0   \n",
       "19  Eine sehr unfreundliche Ärztin, so etwas habe ...     6.0   \n",
       "\n",
       "                                                 text     label  sentiment  \n",
       "1   Dieser Arzt ist das unmöglichste was mir in me...  negative         -1  \n",
       "13   . Termin Stunde Wartezimmer minütige Behandlu...  negative         -1  \n",
       "19  Eine sehr unfreundliche Ärztin, so etwas habe ...  negative         -1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_augm = data_augm.reset_index(drop=True)\n",
    "data_augm.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install torch transformers sentencepiece mosestokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_empty_cache():\n",
    "    \"\"\"Cleans the GPU cache which seems to fill up after a while\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    import torch\n",
    "    import tensorflow as tf\n",
    "\n",
    "    if tf.config.list_physical_devices(\"GPU\"):\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "def get_gpu_device_number():\n",
    "    \"\"\"Provides the number of the GPU device\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The GPU device number of -1 if none is installed\n",
    "    \"\"\"\n",
    "        \n",
    "    import tensorflow as tf\n",
    "    \n",
    "    return 0 if tf.config.list_physical_devices(\"GPU\") else -1\n",
    "\n",
    "def get_compute_device():\n",
    "    \"\"\"Provides the device for the computation\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The GPU device with number (cuda:0) of cpu\n",
    "    \"\"\"\n",
    "        \n",
    "    import tensorflow as tf\n",
    "    \n",
    "    return \"cuda:0\" if tf.config.list_physical_devices(\"GPU\") else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Translation\n",
    "\n",
    "You might repeate following steps for several languages (see [here](https://huggingface.co/models?language=de&pipeline_tag=translation&sort=downloads&search=Helsinki-NLP) for alternative models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**TASK: Try a different language by replacing `lang_to` with another from the [Helsinki-NLP/opus-mt-...](https://huggingface.co/models?language=de&pipeline_tag=translation&sort=downloads&search=Helsinki-NLP) list.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01951ae1c2904254b76b978953d28f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/809k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5404bdfc39304c3c89c0143784d5b87b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/799k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c627acab665f4fb480d883231caffaee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.40M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15294edb6ddb46ba9a4c58155abc8448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa68a087bffb46108eb9eb2075b6d459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c09c2340336405c920391ffd1ad478c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/290M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "477c3dfa05224f0ab89f44983d2ef9a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/799k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a895fba1e9f4978a9f4b46ee4c4e06a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/809k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d875b0da564e70b296940488972e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.40M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d269a8ffe38f4f05819a57f70d186565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc4baf3e194409ebad14b4bba81faf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399b26bfc7174456bb3cdbf070267716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/290M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# replace values to load different tranlsation models\n",
    "lang_from = \"de\"\n",
    "lang_to = \"es\"\n",
    "compute_device = get_compute_device()\n",
    "\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "orig2dest_model_name = \"Helsinki-NLP/opus-mt-\"+lang_from+\"-\"+lang_to\n",
    "orig2dest_tokenizer = MarianTokenizer.from_pretrained(orig2dest_model_name)\n",
    "orig2dest_model = MarianMTModel.from_pretrained(orig2dest_model_name).to(compute_device)\n",
    "dest2orig_model_name = \"Helsinki-NLP/opus-mt-\"+lang_to+\"-\"+lang_from\n",
    "dest2orig_tokenizer = MarianTokenizer.from_pretrained(dest2orig_model_name)\n",
    "dest2orig_model = MarianMTModel.from_pretrained(dest2orig_model_name).to(compute_device)\n",
    "\n",
    "#from transformers import FSMTForConditionalGeneration, FSMTTokenizer\n",
    "#orig2dest_model_name = \"facebook/wmt19-\"+lang_from+\"-\"+lang_to\n",
    "#orig2dest_tokenizer = FSMTTokenizer.from_pretrained(orig2dest_model_name)\n",
    "#orig2dest_model = FSMTForConditionalGeneration.from_pretrained(orig2dest_model_name).to(device)\n",
    "#dest2orig_model_name = \"facebook/wmt19-\"+lang_to+\"-\"+lang_from\n",
    "#dest2orig_tokenizer = FSMTTokenizer.from_pretrained(dest2orig_model_name)\n",
    "#dest2orig_model = FSMTForConditionalGeneration.from_pretrained(dest2orig_model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**TASK: Print the intermediate translations (i.e. decode the `tokenized_dest_texts`) in order to get an understanding of the *creative power* of the back translation (you might want to choose a language you understand).**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_translate_transformers(texts):\n",
    "    #tokenized_texts = orig2dest_tokenizer.prepare_seq2seq_batch(texts, return_tensors=\"pt\").to(compute_device)\n",
    "    tokenized_texts = orig2dest_tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(compute_device)\n",
    "    back_translations = [set() for _ in range(len(texts))]\n",
    "\n",
    "    # Translate texts to target language (e.g. Spanish) and back to source language (e.g. German)\n",
    "    generate_kwargs = {\"num_beams\": 1, \"do_sample\": True, \"num_return_sequences\": 2}\n",
    "    tokenized_dest_texts = orig2dest_model.generate(tokenized_texts[\"input_ids\"], attention_mask=tokenized_texts[\"attention_mask\"], top_p=0.7, **generate_kwargs)\n",
    "    tokenized_source_texts = dest2orig_model.generate(tokenized_dest_texts, top_p=0.8, **generate_kwargs)\n",
    "    \n",
    "    # TODO: !!! place your code here !!!\n",
    "    ####################################\n",
    "    \n",
    "        \n",
    "    ###################\n",
    "    # TODO: !!! end !!!\n",
    "\n",
    "    # Decode and deduplicate back-translations and assign to original text indices\n",
    "    for i, t in enumerate(tokenized_source_texts):\n",
    "        back_translations[i // 4].add(dest2orig_tokenizer.decode(t, skip_special_tokens=True).lower())\n",
    "\n",
    "    # Remove back translations that are empty or equal to the original text\n",
    "    return [[bt for bt in s if bt and bt != t] for s, t in zip(back_translations, map(str.lower, texts))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give it a try..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hallo zusammen. wie geht es euch heute?',\n",
       "  'hallo, leute. wie geht es ihnen heute?',\n",
       "  \"- hallo, allerseits. wie geht's ihnen heute?\",\n",
       "  \"hallo, wie geht's euch heute?\"],\n",
       " ['die nlp ist ja cool, nicht wahr?',\n",
       "  'die nlp ist doch toll, oder?',\n",
       "  'nlp ist cool, oder?',\n",
       "  'nip ist fantastisch, oder?']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "back_translate_transformers([\"Hallo zusammen! Wie geht es euch heute?\", \"NLP ist grossartig, oder?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the actual back translation. Following code allows for recovery in case of a crash..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:48:01.349491 save  9 199\n",
      "13:48:47.050349 save  19 399\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-6590e016e032>\u001b[0m in \u001b[0;36mback_translate_transformers\u001b[0;34m(texts)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Translate texts to German and back to English\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mgenerate_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"num_beams\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"do_sample\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"num_return_sequences\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtokenized_de_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig2dest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenized_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtokenized_en_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdest2orig_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_de_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1025\u001b[0m                 \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m                 \u001b[0msynced_gpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m             )\n\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1566\u001b[0m             \u001b[0;31m# sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m             \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m             \u001b[0mnext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m             \u001b[0;31m# finished sentences should have their next token be a padding token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from datetime import datetime\n",
    "\n",
    "batch_size = 5\n",
    "save_every_n_elements = 50\n",
    "translations = []\n",
    "last_stored = -1 #8409\n",
    "\n",
    "# set to the last stored index for recovery\n",
    "if last_stored >= 0:\n",
    "    data_trans = load_dataframe(\"data/german_doctor_reviews_augmented_tmp.parq\")\n",
    "    translations = [row.to_dict() for index, row in data_trans.iterrows()]\n",
    "    print(\"Loaded\", len(translations))\n",
    "\n",
    "for g, df in data_augm.groupby(np.arange(len(data_augm)) // batch_size):\n",
    "    if g > last_stored:\n",
    "        gpu_empty_cache()\n",
    "        back_trans = back_translate_transformers(df[\"text_original\"].to_list())\n",
    "    \n",
    "        i = 0\n",
    "        for index, row in df.iterrows():\n",
    "            \n",
    "            for trans in back_trans[i]:\n",
    "                row_dict = row.to_dict()\n",
    "                row_dict[\"text\"] = trans\n",
    "                translations.append(row_dict)\n",
    "           \n",
    "            i += 1\n",
    "    \n",
    "        if (g + 1) % (save_every_n_elements // batch_size) == 0:\n",
    "            print(datetime.now().time(), \"save \", g, len(translations))\n",
    "        \n",
    "            save_dataframe(pd.DataFrame(translations), \"data/german_doctor_reviews_augmented_tmp.parq\")\n",
    "            \n",
    "    else:\n",
    "        print(\"Skip\", g)\n",
    "              \n",
    "    \n",
    "save_dataframe(pd.DataFrame(translations), \"data/german_doctor_reviews_augmented_tmp.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data = pd.DataFrame(translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_original</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dieser Arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>dieser arzt ist das unmöglichste, das ich je i...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dieser Arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>dieser arzt ist das unmöglichste, was ich jema...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dieser Arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>dieser arzt ist am wenigsten unmöglich in mein...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       text_original  rating  \\\n",
       "0  Dieser Arzt ist das unmöglichste was mir in me...     6.0   \n",
       "1  Dieser Arzt ist das unmöglichste was mir in me...     6.0   \n",
       "2  Dieser Arzt ist das unmöglichste was mir in me...     6.0   \n",
       "\n",
       "                                                text     label  sentiment  \n",
       "0  dieser arzt ist das unmöglichste, das ich je i...  negative         -1  \n",
       "1  dieser arzt ist das unmöglichste, was ich jema...  negative         -1  \n",
       "2  dieser arzt ist am wenigsten unmöglich in mein...  negative         -1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataframe(save_data, \"data/german_doctor_reviews_augmented_translated_\"+lang_to+\".parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all the back translated text and perform normalization of the augmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/german_doctor_reviews_augmented_translated_es.parq']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "files = glob.glob(\"data/german_doctor_reviews*augmented_trans*_[a-z][a-z].parq\")\n",
    "print(files)\n",
    "\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    data_aug = load_dataframe(file)\n",
    "    dataframes.append(data_aug)\n",
    "    \n",
    "data_aug = pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131629, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_aug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_original</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dieser Arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>dieser arzt ist das unmöglichste, das ich je i...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dieser Arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>dieser arzt ist das unmöglichste, was ich jema...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dieser Arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>dieser arzt ist am wenigsten unmöglich in mein...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       text_original  rating  \\\n",
       "0  Dieser Arzt ist das unmöglichste was mir in me...     6.0   \n",
       "1  Dieser Arzt ist das unmöglichste was mir in me...     6.0   \n",
       "2  Dieser Arzt ist das unmöglichste was mir in me...     6.0   \n",
       "\n",
       "                                                text     label  sentiment  \n",
       "0  dieser arzt ist das unmöglichste, das ich je i...  negative         -1  \n",
       "1  dieser arzt ist das unmöglichste, was ich jema...  negative         -1  \n",
       "2  dieser arzt ist am wenigsten unmöglich in mein...  negative         -1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_aug.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fhnw.nlp.utils.normalize import tokenize\n",
    "from fhnw.nlp.utils.normalize import tokenize_stem\n",
    "from fhnw.nlp.utils.normalize import tokenize_lemma\n",
    "from fhnw.nlp.utils.normalize import normalize\n",
    "from fhnw.nlp.utils.text import clean_text\n",
    "from fhnw.nlp.utils.text import join_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy>=3.0.5 in /usr/local/lib/python3.6/dist-packages (3.1.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.6/dist-packages (from spacy>=3.0.5) (1.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=3.0.5) (1.19.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=3.0.5) (21.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from spacy>=3.0.5) (3.0.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=3.0.5) (57.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=3.0.5) (2.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=3.0.5) (0.7.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.6/dist-packages (from spacy>=3.0.5) (2.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.6/dist-packages (from spacy>=3.0.5) (0.6.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=3.0.5) (1.0.5)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from spacy>=3.0.5) (3.7.4.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=3.0.5) (4.62.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=3.0.5) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=3.0.5) (0.8.2)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=3.0.5) (0.3.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=3.0.5) (3.0.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy>=3.0.5) (8.0.10)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=3.0.5) (2.26.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=3.0.5) (2.4.1)\n",
      "Requirement already satisfied: dataclasses>=0.6; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy>=3.0.5) (0.8)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=20.0->spacy>=3.0.5) (2.4.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.6/dist-packages (from jinja2->spacy>=3.0.5) (2.0.1)\n",
      "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy>=3.0.5) (3.5.0)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from pathy>=0.3.5->spacy>=3.0.5) (5.2.1)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.6/dist-packages (from typer<0.4.0,>=0.3.0->spacy>=3.0.5) (7.1.2)\n",
      "Requirement already satisfied: contextvars<3,>=2.4; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from thinc<8.1.0,>=8.0.8->spacy>=3.0.5) (2.4)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.5) (2.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.5) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.5) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.5) (2021.5.30)\n",
      "Requirement already satisfied: immutables>=0.9 in /usr/local/lib/python3.6/dist-packages (from contextvars<3,>=2.4; python_version < \"3.7\"->thinc<8.1.0,>=8.0.8->spacy>=3.0.5) (0.16)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.6.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from nltk) (2021.8.28)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from nltk) (4.62.2)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from nltk) (7.1.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-01 00:28:04.728499: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "Requirement already satisfied: de-core-news-lg==3.1.0 from https://github.com/explosion/spacy-models/releases/download/de_core_news_lg-3.1.0/de_core_news_lg-3.1.0-py3-none-any.whl#egg=de_core_news_lg==3.1.0 in /usr/local/lib/python3.6/dist-packages (3.1.0)\n",
      "Requirement already satisfied: spacy<3.2.0,>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from de-core-news-lg==3.1.0) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (3.7.4.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (57.4.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (3.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (2.26.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (0.8.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (2.4.1)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (0.3.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (2.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (1.19.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (8.0.10)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (4.62.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (1.8.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (3.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (21.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (2.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (0.6.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (0.7.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (2.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (2021.5.30)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.6/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (7.1.2)\n",
      "Requirement already satisfied: contextvars<3,>=2.4; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from thinc<8.1.0,>=8.0.8->spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (2.4)\n",
      "Requirement already satisfied: dataclasses<1.0,>=0.6; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from thinc<8.1.0,>=8.0.8->spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (0.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.6/dist-packages (from jinja2->spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (2.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (3.5.0)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (5.2.1)\n",
      "Requirement already satisfied: immutables>=0.9 in /usr/local/lib/python3.6/dist-packages (from contextvars<3,>=2.4; python_version < \"3.7\"->thinc<8.1.0,>=8.0.8->spacy<3.2.0,>=3.1.0->de-core-news-lg==3.1.0) (0.16)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_lg')\n"
     ]
    }
   ],
   "source": [
    "!pip install 'spacy>=3.0.5'\n",
    "!pip install nltk\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import spacy\n",
    "!python3 -m spacy download de_core_news_md\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "\n",
    "stemmer = SnowballStemmer(\"german\")\n",
    "empty_stopwords = set()\n",
    "stopwords = set(stopwords.words(\"german\"))\n",
    "n_cores = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 725 ms, sys: 416 ms, total: 1.14 s\n",
      "Wall time: 2.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# make sure we did not introduce maleformated stuff\n",
    "data_aug = data_aug.rename(columns={\"text\": \"text_tmp\"})\n",
    "data_aug = parallelize_dataframe(data_aug, clean_text, n_cores=n_cores, field_read=\"text_tmp\", field_write=\"text\", keep_punctuation=True)\n",
    "data_aug = data_aug.drop(columns=[\"text_tmp\"], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.1 s, sys: 112 ms, total: 6.21 s\n",
      "Wall time: 6.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "save_dataframe(data_aug, \"data/german_doctor_reviews_augmented_tokenized_tmp.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.29 s, sys: 656 ms, total: 4.95 s\n",
      "Wall time: 31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_aug = parallelize_dataframe(data_aug, normalize, n_cores=n_cores, field_read=\"text\", field_write=\"token_clean\", stopwords=empty_stopwords, stemmer=None, lemmanizer=None, lemma_with_ner=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.3 s, sys: 204 ms, total: 10.5 s\n",
      "Wall time: 10.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "save_dataframe(data_aug, \"data/german_doctor_reviews_augmented_tokenized_tmp.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.2 s, sys: 1.18 s, total: 32.4 s\n",
      "Wall time: 33.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_aug = parallelize_dataframe(data_aug, join_tokens, n_cores=n_cores, field_read=\"token_clean\", field_write=\"text_clean\", stopwords=empty_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.7 s, sys: 312 ms, total: 15 s\n",
      "Wall time: 14.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "save_dataframe(data_aug, \"data/german_doctor_reviews_augmented_tokenized_tmp.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min, sys: 6.34 s, total: 1min 6s\n",
      "Wall time: 3min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_aug = parallelize_dataframe(data_aug, normalize, n_cores=n_cores, field_read=\"token_clean\", field_write=\"token_lemma\", stopwords=stopwords, stemmer=None, lemmanizer=nlp, lemma_with_ner=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.9 s, sys: 308 ms, total: 16.2 s\n",
      "Wall time: 16.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "save_dataframe(data_aug, \"data/german_doctor_reviews_augmented_tokenized_tmp.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33 s, sys: 1.59 s, total: 34.6 s\n",
      "Wall time: 47.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_aug = parallelize_dataframe(data_aug, normalize, n_cores=n_cores, field_read=\"token_clean\", field_write=\"token_stem\", stopwords=stopwords, stemmer=stemmer, lemmanizer=None, lemma_with_ner=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.4 s, sys: 384 ms, total: 18.7 s\n",
      "Wall time: 18.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "save_dataframe(data_aug, \"data/german_doctor_reviews_augmented_tokenized_tmp.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.2 s, sys: 1.45 s, total: 35.6 s\n",
      "Wall time: 42.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_aug = parallelize_dataframe(data_aug, normalize, n_cores=n_cores, field_read=\"token_clean\", field_write=\"token_clean_stopwords\", stopwords=stopwords, stemmer=None, lemmanizer=None, lemma_with_ner=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.8 s, sys: 396 ms, total: 21.1 s\n",
      "Wall time: 21.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "save_dataframe(data_aug, \"data/german_doctor_reviews_augmented_tokenized_tmp.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aug = data_aug[data_aug[\"token_lemma\"].map(len) > 1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_original</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>token_clean</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>token_stem</th>\n",
       "      <th>token_clean_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dieser Arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "      <td>dieser arzt ist das unmöglichste, das ich je i...</td>\n",
       "      <td>[dieser, arzt, ist, das, unmöglichste, ,, das,...</td>\n",
       "      <td>dieser arzt ist das unmöglichste , das ich je ...</td>\n",
       "      <td>[arzt, unmöglichste, je, leben, triefen, böswi...</td>\n",
       "      <td>[arzt, unmog, ,, je, leb, getroff, ,, boswill,...</td>\n",
       "      <td>[arzt, unmöglichste, ,, je, leben, getroffen, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dieser Arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "      <td>dieser arzt ist das unmöglichste, was ich jema...</td>\n",
       "      <td>[dieser, arzt, ist, das, unmöglichste, ,, was,...</td>\n",
       "      <td>dieser arzt ist das unmöglichste , was ich jem...</td>\n",
       "      <td>[arzt, unmöglichste, jemals, leben, kennen, ve...</td>\n",
       "      <td>[arzt, unmog, ,, jemal, leb, kannt, ,, versaut...</td>\n",
       "      <td>[arzt, unmöglichste, ,, jemals, leben, kannte,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dieser Arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "      <td>dieser arzt ist am wenigsten unmöglich in mein...</td>\n",
       "      <td>[dieser, arzt, ist, am, wenigsten, unmöglich, ...</td>\n",
       "      <td>dieser arzt ist am wenigsten unmöglich in mein...</td>\n",
       "      <td>[arzt, wenig, unmöglich, leben, finden, unfreu...</td>\n",
       "      <td>[arzt, wenig, unmog, leb, find, ,, unfreund, ,...</td>\n",
       "      <td>[arzt, wenigsten, unmöglich, leben, finden, ,,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       text_original  rating     label  \\\n",
       "0  Dieser Arzt ist das unmöglichste was mir in me...     6.0  negative   \n",
       "1  Dieser Arzt ist das unmöglichste was mir in me...     6.0  negative   \n",
       "2  Dieser Arzt ist das unmöglichste was mir in me...     6.0  negative   \n",
       "\n",
       "   sentiment                                               text  \\\n",
       "0         -1  dieser arzt ist das unmöglichste, das ich je i...   \n",
       "1         -1  dieser arzt ist das unmöglichste, was ich jema...   \n",
       "2         -1  dieser arzt ist am wenigsten unmöglich in mein...   \n",
       "\n",
       "                                         token_clean  \\\n",
       "0  [dieser, arzt, ist, das, unmöglichste, ,, das,...   \n",
       "1  [dieser, arzt, ist, das, unmöglichste, ,, was,...   \n",
       "2  [dieser, arzt, ist, am, wenigsten, unmöglich, ...   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  dieser arzt ist das unmöglichste , das ich je ...   \n",
       "1  dieser arzt ist das unmöglichste , was ich jem...   \n",
       "2  dieser arzt ist am wenigsten unmöglich in mein...   \n",
       "\n",
       "                                         token_lemma  \\\n",
       "0  [arzt, unmöglichste, je, leben, triefen, böswi...   \n",
       "1  [arzt, unmöglichste, jemals, leben, kennen, ve...   \n",
       "2  [arzt, wenig, unmöglich, leben, finden, unfreu...   \n",
       "\n",
       "                                          token_stem  \\\n",
       "0  [arzt, unmog, ,, je, leb, getroff, ,, boswill,...   \n",
       "1  [arzt, unmog, ,, jemal, leb, kannt, ,, versaut...   \n",
       "2  [arzt, wenig, unmog, leb, find, ,, unfreund, ,...   \n",
       "\n",
       "                               token_clean_stopwords  \n",
       "0  [arzt, unmöglichste, ,, je, leben, getroffen, ...  \n",
       "1  [arzt, unmöglichste, ,, jemals, leben, kannte,...  \n",
       "2  [arzt, wenigsten, unmöglich, leben, finden, ,,...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_aug.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#save_dataframe(data_aug, \"data/german_doctor_reviews_augmented_tokenized.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
